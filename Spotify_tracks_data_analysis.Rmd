---
title: "Spotify_Data_Track_Analysis"
author: "AvaniPatil, Sushma Akoju, Armana Anand"
date: "9/23/2021"
output: html_document
---

```{r libraries}
options(scipen=999)

# Importing the libraries

knitr::opts_chunk$set(echo = TRUE, fig.width=30, fig.height=20)
library(tidyverse)
library(dplyr)
library(glue)
library(ggplot2)
library(ggcorrplot)
library(broom)
library(lubridate)
library(viridis)
library(GGally)
library(scales)
library(readr)
library(corrplot)
library(randomForest) # basic implementation
library(purrr)
library(corrr)
library(vtree)
library(reshape)
library(modelsummary)
library(gt)
library(kableExtra)
library(funModeling)
library(rpart)

library("permimp")
library("party", quietly=TRUE)
```

# Objective 

Spotify is the worlds largest audio streaming application with services available in more than 175 countries. With a market share of approximately 32%, it has  365 million monthly active users, including 165 million paying subscribers, as of June 2021. A user can search for music based on a song, artist and genre album. They can create, share, edit playlists with other listeners all over the world. With such a massive user base, as one would imagine Spotify data is a gold mine for training machine learning algorithms and recommendation based systems. As a freemium service, Spotify implements multitudes of data learning tools and algorithms to leverage its data and create a streamlined experience for its users, unmatched by its competitors. Our objective with this project is to gain a deeper understanding of the Data Science discipline by accessing this data, and doing some preliminary analysis to come up with some conclusive observations.

We will primarily focus and attempt to understand each of the features, their technical definitions cited here https://developer.spotify.com/documentation/web-api/reference/#endpoint-get-audio-features . And we would like to see how existing human notions about music fare against what data actually tells us about. For example, it has been widely popular that the C# chord is most popular in western music. We want to find out how the features contribute to popularity.


## Organization of the Report
* Objective
* Methodology
  + Web Data Collection
  + Pre-processing
* Analysis and Visualizations
  + Popularity of each genre
  + Correlation Map
  + Correlation coefficient for each feature
  + Popularity for each key
  + Correlation based on feature groups
  + Spotify tracks by Genre in the US
  + Data summary and density distribution for all Spotify features
  + Permutation Importances
  
* Modeling the Data
  + Linear Regression Modelling to predict popularity
  + Linear Regression Modelling to check the linear fit
  + Random Forest Regression Fit
  
  
* Potential Bias and Conclusion

# Methodology 

In this section, the process of data gathering and cleaning are discussed.

## Web Data Collection
```{r}
#Importing the data set

tracks <- read_csv("https://raw.githubusercontent.com/sushmaakoju/spotify-tracks-data-analysis/main/SpotifyFeatures.csv")

head(tracks,n=10)
colnames(tracks)

```

## Armana Anand Code

Downloaded from Kaggle, this data was originally sourced from Spotify using their API. 

[Here is the link to the Kaggle dataset.](https://www.kaggle.com/tomigelo/spotify-audio-features)

The data set consists of the features of around 200k  top songs—Worldwide. Ranked by a proprietary algorithm for popularity in 2019.

Here are the features which will be measured to evaluate the data.

* **Genre**
* **Artist_name**
* **Track_name**
* **Track_id**
* **Popularity**: Scaled scaled from 0-100(Least –Most Popular)
* **Acousticness**: Measure of how acoustic the track is. Ranges from 0.0 to 1.0
* **Danceability**: Measured using a mixture of song features such as beat strength, tempo stability, and   overall tempo. 
* **Duration_ms**
* **Energy**: Measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy.
* **Instrumentalness**: Does it contain vocals? Range 0-1.
* **Key**
* **Liveness**: Detects the presence of an audience in the recording. 
* **Loudness**: Overall loudness of a track in decibels (dB). Values range between -55 and 5 dB.
* **Mode**
* **Speechiness**: Detects the presence of spoken words in a track.
* **Tempo**: Overall estimated tempo of a track in beats per minute (BPM).
* **Time_signature**
* **Valence**: Measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track


More in depth information on how Spotify tags and analyses these features can be found [here]("https://community.spotify.com/t5/Spotify-for-Developers/") 


## Pre-processing
Let's check for missing values in our data set.

```{r missingvalues}
#checking for missing values in the data set

colSums(is.na(tracks))
```

As per above summary, there are no missing values in this data set.

```{r distinctvalues}
# As there are no missing values, we will check for other errors.

# Let's check for distinct genre names to see if any genre is repeated.

distinct(data.frame(tracks$genre)) 
```

As we can see there are two Children's Music genre. We should rectify this typo so that a single genre is visible.

```{r cleandata}

# Rectifying typo in genre name Children's Music.

colnames(tracks)[colnames(tracks$genre) == "Children's Music"] <- "Children’s Music"

# Rechecking the distinct values.

distinct(data.frame(tracks$genre))
```

```{r mutate}
#Mutate variables from numeric to factor

tracks <- tracks  %>% 
  mutate(genre = as.factor(genre),
         key = as.factor(key),
         genre = as.factor(str_replace_all(genre, "[[:punct:]]", "")),
         mode = as.factor(mode))

summary(tracks)
```

```{r filterdata}

# Filter columns that are not presently needed for our analysis

tracks1 <- tracks %>% select(-c(track_id,time_signature))

# Glimpse data

glimpse(tracks1)

```

Before we can move to visualizations and deeper analysis, lets take a look at what the overall trend of our data is.

```{r}
spotify_hist <- tracks[,-c(1,2,3,4,5,11,17)]
plot_num(spotify_hist)

```

From the above it is interesting to note that danceability has a pretty normal distribution of songs in our dataset. To no surprise, there are not many people listening to Bach or YoYo Ma renditions of Vivaldi off late. We can clearly see the evidence of that by looking at the  tall peak that signifies the clustering of most of our songs on the low to zero instrumental scale. Similarly, majority of the popular songs generally are 2- 5 mins long.  


# Analysis and Visualizations

Now that we have pre-processed our data set, we can visualize basic data to find correlations among our variables.

From our data set, we can group our data set by genre and focus on the popularity parameter for visualization of data.

```{r plot_averagepopularity_genre}
genre_popularity <- tracks %>% select(popularity, genre) %>% group_by(genre) %>% summarise("average_popularity" = round(mean(popularity)))

p <- ggplot(data=genre_popularity, mapping = aes(x = reorder(genre,average_popularity), y = average_popularity, fill = genre)) +
  geom_col() +
  coord_flip() +
  theme_minimal() +
  theme(
    legend.position = "none",
    
  ) +
  labs(
    y = "Average popularity",
    x = "Genre"
  )

p
```
We have plotted the music genre from highest popularity to lowest popularity.

We can see that the Top 5 popular genres on spotify are -
1. Pop 
2. Rap
3. Rock
4. HipHop
5. Dance

Genres with lowest popularity on spotify.

1. Anime 
2. Comedy 
3. Opera 
4. Movie 
5. A Capella

Let's analyse the relationship between all numeric Spotify features now using a correlation map.

```{r plotcorr, warning=FALSE}
# Plotting a correlation heat map to check correlation between all audio features.

ggcorr(tracks, low = "blue", high = "red")
```

From above graph we can list out below significant observations -

1) Energy and loudness have the highest correlation.

2) Energy and acousticness have a highly-correlated inverse relationship

3) Loudness and acousticness have a highly-correlated inverse relationship

4) Some of the variables most correlated to Popularity seem to be: acousticness, danceability, energy, instrumentalness, and loudness.


## Avani Patil Code

Now that we are familiar with the data set attributes and the objective of our analysis, let's check for missing values in our data set.

```{r missingvalues_spotify}

# Checking for missing values in the data set

colSums(is.na(tracks))
```

As per above summary, there are no missing values in this data set.

Now that we have the pre-processed data set, we can visualize basic data to find correlations among our variables.

In our data set, we will be focusing on Popularity as our dependent variable. 
So how is popularity actually calculated and what makes a song popular?

According to Spotify - popularity is calculated by an algorithm and is based, in the most part, on the total number of plays the track has had and how recently those tracks are played. Generally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past.

So for starters, let's check the relationship between all the audio features of Spotify data set to try and find some correlation between them using th correlation heatmap mentioned above.

From above mentioned heatmap we can summarize that -

1) ‘Energy’ and ‘loudness’ have the highest correlation, and a positive one, which is not very surprising as the louder a song is, the more energy it has.

2) ‘Energy’ and ‘acousticness’ have a highly-correlated inverse relationship, which also makes total sense. The more a song skews towards being acoustic, the less energy value it tends to have.
We noticed similar inverse relationship between ‘Loudness’ and ‘acousticness’ as well.

Let's sort the the data by popularity to check the top 10 popular songs.

```{r}
tracks1 %>% top_n(10,popularity) %>% select(artist_name, track_name, popularity) %>% arrange(desc(popularity))
```


## Correlation coefficient for each feature

Now that we have plotted the correlation heat map, let's check the correlation coefficient values against each relationship.

```{r PearsonCorr}

# Eliminate the parameters which are not relevant to our correlation analysis and copy the same in a new df. 

tracks2 <- tracks %>% select(-c(track_id,time_signature,key,mode))

cor(tracks2[,4:14])

```

With our dependent variable being ‘popularity’, from above graph we can note that there are poor correlation values across most of our independent variables.
From this correlation matrix, we can pluck five of the best features (ones with the highest correlation) to use later on as predictors while training our model -
acousticness, 
danceability, 
energy, 
instrumentalness, and 
loudness.

## Popularity for each key

Now let's compare Popularity with Key and check if we can find any significant relationship between these two attributes.

So when it comes to western music, there are 12 keys. Let's check the popularity for each and every key.

```{r}

# Plot keys and their popularity values.

tracks %>% group_by(key) %>% select(key, popularity) %>%
  ggplot(aes(x=as.factor(key),y=popularity,fill=as.factor(key))) +
  geom_boxplot() + theme(legend.position = "none") +
  labs(title="Popularity of Song Keys",
       x="Key", y="Popularity")
```

There seems to be little differentiation between the keys and popularity. However, one key does seem to have a larger number of popular songs in it - C-Sharp key.

We can check this by filtering out songs where popularity is above 70 on a scale on 100 and check how many popular songs fall under this key.


```{r}
#tracks %>% filter(popularity > 70) %>% group_by(key) %>% summarize(count = n()) %>% arrange(desc(count))
```
Key 1 (which applies to c-Sharp) has more popular songs (1068 songs) than other keys. This could be a potential predictor variable, so we will encode a new binary variable we can use in our model.

```{r}
#  Creating a new binary variable for key attribute

tracks$isKey1 <- as.integer(tracks$key == 1)
```

## Linear Regression Modelling

To warm-up, we will be starting with a simple linear regression model and try and manipulate the predictor attributes to find the a model with lowest Residual standard error and highest Adjusted R-squared.

Linear Model 1 (m1):

Dependent Variable - Popularity
Predictor Variables - acousticness danceability, energy, instrumentalness, liveness, loudness, speechiness, tempo, valence, isKey1, energy and loudness.

```{r lm1}
m1 <- lm(popularity ~ acousticness + danceability + energy + instrumentalness + liveness + loudness + speechiness + tempo + valence + isKey1 + (energy * loudness), data=tracks)

summary(m1)
```

From above model, we can observe that the Adjusted R-squared is very low and the Residual standard error is on the higher end for this model.
Also, the key attribute is not adding up to anything in our model. 

In our next modelling attempt, let's get rid of the key attribute and add couple of more parameters which have shown correlation among them.


Linear Model 2 (m2):

Dependent Variable - Popularity
Predictor Variables - acousticness danceability, energy, instrumentalness, liveness, loudness, speechiness, tempo, valence, energy and loudness, acousticness and instrumentalness.

In this model, we have added an additional predictor acousticness and instrumentalness.

```{r lm2}
m2 <- lm(popularity ~ acousticness + danceability + energy + instrumentalness + liveness + loudness + speechiness + tempo + valence + (energy * loudness) + (acousticness * instrumentalness), data=tracks)

summary(m2)
```

In above model, we can see that the Adjusted R-squared is has increased from 0.2338 to 0.2456 and the Residual standard error has decreased from 15.92 to 15.8.
However, the updated Adjusted R-squared is still quite low. 

Let's try and improve the model further by adding for correlation predictors.

Linear Model 3 (m3):

Dependent Variable - Popularity
Predictor Variables - acousticness danceability, energy, instrumentalness, liveness, loudness, speechiness, tempo, valence, energy and loudness, acousticness and instrumentalness, energy and danceability.

In this model, we have added an additional predictor energy and danceability.

```{r}
m3 <- lm(popularity ~ acousticness + danceability + energy + instrumentalness + liveness + loudness + speechiness + tempo + valence + (energy * loudness) + (acousticness * instrumentalness) + (energy * danceability), data=tracks)

summary(m3)
```

In above model, we can see that the Adjusted R-squared is has increased from 0.2456 to 0.2464 and the Residual standard error has decreased from 15.8 to 15.79.

We’ll chose the model m3 as it has the best fit (R-Squared) to the data compared to other models.

Let's plot model m3 using residual plot and QQ plot.

```{r plotresidual}
# Plotting residual plot for Model 3

am3 <- augment(m3)

am3 %>% ggplot(aes(x=.fitted,y=.resid)) +
  geom_point(alpha=0.1) + geom_hline(yintercept=0) +
  labs(title="Residual Plot")
```

```{r plotQQ}

# Plotting QQ plot for Model 3

am3 %>% ggplot(aes(sample=.std.resid)) +
  geom_qq() + geom_qq_line() +
  labs(title="QQ Plot")
```


The residual plot has a definite pattern however, the QQ plot shows that our residuals aren’t exactly normal throughout the range of samples.

Given the low Adjusted R-squared, and unusual patterns in the residuals, the models we’ve created seem like they are unsuitable for predicting a song’s popularity on Spotify.

## Sushma Akoju Code

Goal :
We primarily focus and attempt to understand each of the features, their technical definitions cited here https://developer.spotify.com/documentation/web-api/reference/#endpoint-get-audio-features . we would like to see how existing human notions about music fare against what data actually tells us about. For example, it has been widely popular that the C# chord is most popular in western music. We want to find out how the features contribute to popularity. 

Example: "My favorite things" song from "The Sound of music" movie which was very popular back in 1960s and still considered a classic is unfortunately has a popularity score of zero while a modern, contemporary version inspired from the same song and rewritten with different lyrics and named as "7 rings" by Ariana Grande, has a popularity score of 100.

##### Get data from url to a Tibble.

```{r }
spotifydf <- tracks
head(spotifydf)
colSums(is.na(spotifydf))
head( spotifydf)
glimpse(spotifydf)
```

## Including Plots
Some genres are duplicated. (encoding mismatches).

```{r}
na.omit(spotifydf)
genres <- distinct(spotifydf, genre)$genre
genres
spotifydf[!duplicated(spotifydf$track_id),]
genres <- distinct(spotifydf, genre)$genre
```

Convert character format columns: key and mode to numeric values. 

```{r}
unique(spotifydf$key)
unique(as.numeric(as.factor(spotifydf$key)))
spotifydf$key <- as.numeric(as.factor(spotifydf$key))

unique(spotifydf$mode)
unique(as.numeric(as.factor(spotifydf$mode)))
spotifydf$mode <- as.numeric(as.factor(spotifydf$mode))

```


Get all numeric columns from the dataframe.

```{r}
str(spotifydf)
columns <- colnames(spotifydf)
columns
numeric_columns <- unlist(lapply(spotifydf, is.numeric))
numeric_columns
numeric_spotifydf <- spotifydf[,numeric_columns]
colnames(numeric_spotifydf)
```

## Correlation based on feature groups

Use Corrr library to plot correlation based on feature groups.

Found this library more helpful to group into clusters for higher or similar correlation between features.

```{r}

corr_df <- correlate(numeric_spotifydf, quiet = TRUE)
corr_df
corr_df %>% 
  select(-term) %>% 
  map_dbl(~ mean(., na.rm = TRUE))

corr_df2 <- cor(numeric_spotifydf)
col3 = hcl.colors(10, "YlOrRd", rev = TRUE)
corr1 <- corrplot(corr_df2, col=col3, method = 'number') 

corrplot(corr_df2,  order = 'hclust', addrect = 2)
corrplot(corr_df2/2, col.lim=c(-0.5, 0.5))
```


There is a high correlation between energy and loudness features and similarly there is a second highest correlation between valence and danceability where valence is positiveness or negativeness of a track defined by (cheerful vs sad, depressive tune, lyrics)

## Spotify tracks by Genre in the US

For each feature, plot grouping w.r.t Popularity.

```{r}

genres_df <- spotifydf %>%
   select(popularity, genre)%>%
    count(popularity, genre)
by_genres <- spotifydf %>% group_by(genre, popularity)
by_genres %>% summarise(
  popularity = mean(popularity),
)
by_genre <- by_genres %>% summarise(n = n())
by_genre %>% summarise(n = sum(n)) %>% filter(n>0)
#vtree(genres_df, "genre")
colnames(by_genre)

ggplot(genres_df)+
  geom_point(aes(x = genre, y= n))

genres_df %>% ggplot(aes(x = genre, y= n))+
  geom_line(aes(color = "genre")) 

ggplot(data=by_genre,aes(x = genre, y = n, fill=genre)) +
  geom_bar(stat="identity", width=0.5,  position=position_dodge())+
coord_flip()+
    labs(title = "Spotify tracks by Genre in US", y= NULL)
```

Group by Genres and select all features except text based features.

```{r}

genres_df <- spotifydf %>%
  select(-c(artist_name, track_name, track_id, time_signature, key))

```


## Data summary and density distribution for all spotify features

Each of features do seem to have different types of density, suggesting distributions are different from each other. It would have been nice if some normalization technique or re-sampling of features was done. But in the interest of time, we could not do this. From the above density plots, it would be reaosnable to find some kind of normalization between feature data.

```{r}
spotify_summary <- datasummary_skim(numeric_spotifydf)
spotify_summary
```

Plot the density distributions of each of features.

```{r}
spotify_histograms <- numeric_spotifydf[,-c(4)]
plot_num(spotify_histograms)

ggplot(spotifydf, aes(popularity)) +
  geom_density()
ggplot(spotifydf, aes(energy)) +
  geom_density()
ggplot(spotifydf, aes(danceability)) +
  geom_density()
ggplot(spotifydf, aes(key)) +
  geom_density()
```


## Linear Regression Modelling to check the linear fit

Check linear fit between all features vs popularity
This is multiple regression since we have multiple predictors vs one response variable.

We find a linear fit for y = [popularity] and X = [ acousticness, danceability, duration_ms, energy, instrumentalness, liveness, loudness, speechiness, tempo, valence] to check model fit.

```{r}

lmfit = lm(popularity ~ acousticness + danceability + duration_ms +energy + instrumentalness + liveness + loudness + speechiness + tempo + valence, numeric_spotifydf )
summary(lmfit)
plot(lmfit)
```

The summary of linear fit suggests that R-squared value is 0.2339 which is not significant enough to explain how much percentage of data fits linearly. Standard error is 15.92 suggests this is 15.92 standard deviations of the residuals away from true regression fit. The p-value of less than 0.01 suggests that null hypothesis that predictors and response variables are not related can be rejected. Hence observed values of response variable are no better than predicted values, by a chance. However since this is a multiple linear regression, F-statistic may also be relevant here since it checks if atleast one of the predictors' coefficients is non-zero. F-statistic = (SSR/SSE) = (Sum of squares regression) / (sum of squares error) -> is the ratio of variance explained / varinace that cannot be explained. The standard errors suggest that about ~75% of variance in data cannot be explained. P(> |t|) for each of features also suggests null hypothesis that there is no relation between predictors and response variable - must be rejected.

checking coefficients

```{r}
coefficients(lmfit)
```


```{r}

lmfit2 = lm(popularity ~ acousticness + danceability +energy + instrumentalness + liveness + loudness + speechiness + tempo + valence + key+mode+(energy * loudness) + (valence * danceability) , numeric_spotifydf )
summary(lmfit2)
```

The summary of linear fit suggests that R-squared value is 0.2392 which should be significant enough to explain how much percentage of data fits linearly. Standard error is 15.92 suggests this is 15.92 standard deviations of the residuals away from true regression fit. The p-value of less than 0.01 suggests that null hypothesis that predictors and response variables are not related can be rejected. Hence observed values of response variable are no better than predicted values, by a chance. However since this is a multiple linear regression, F-statistic may also be relevant here since it checks if atleast one of the predictors' coefficients is non-zero. F-statistic = (SSR/SSE) = (Sum of squares regression) / (sum of squares error) -> is the ratio of variance explained / varinace that cannot be explained. The standard errors suggest that about ~75% of variance in data cannot be explained. P(> |t|) for each of features also suggests null hypothesis that there is no relation between predictors and response variable - must be rejected. From p(>|t|) = 0.08 for valence, seems to indicate null hypothesis true for valence and popularity score suggesting that valence has no influence on popularity score.

```{r}
anova(lmfit2)
```


```{r}
plot(lmfit2)
```

get model summary for Linear regression and Generalized Linear regression with Gaussian.

```{r}
models <- list(
  "OLS"     = lmfit2,
  "GLM" = glm(popularity ~ acousticness + danceability +energy + instrumentalness + liveness + loudness + speechiness + tempo + valence + key+mode+(energy * loudness) + (valence * danceability) , data = numeric_spotifydf , family = gaussian)
)
modelsummary(models,
             fmt = 1,
               estimate  = c(
                "{estimate} ({std.error})",
                "{estimate} [{conf.low}, {conf.high}]"),
               statistic = NULL,
              coef_omit = "Intercept",
             output = "table1.png")


```

Get standard errors, statistics and p-values for each of models.

```{r}
modelsummary(models,gof_omit = ".*",
               statistic = c("conf.int",
                           "s.e. = {std.error}", 
                           "t = {statistic}",
                           "p = {p.value}"),
             output = "table2.png")
```

taking linear regression fit analysis further, using model summary package we are able to compare how features fit in comparison between linear regression as well as Gaussian Generalized Linear regression fit. There is no change in P-values or null hypothesis analysis. However, individual feature's standard errors seems significant and are very low. This suggests that linear regression (generalized or OLS) seem to be overfitted.


## Random Forest Regression Fit

Random Forest Regression fit by train and test split.
Create train and test sets

```{r}
train = sample(1:nrow(numeric_spotifydf), 300)
rf.spotify = randomForest(popularity~., data = numeric_spotifydf, subset = train)
rf.spotify
```

In case of Random Forest regression, number of trees are 500 with 4 variables at each split. as well the MSR is 273.81 and explained variance is still low 27.88 % of variance explained.
The following plot suggests that 50 trees or so is enough to fit the model with RFR.

```{r}
plot(rf.spotify)

```

For each variables 1 to 17 of features, find Out-of-bag and test errors for each fit.

```{r}
oob.err = double(17)
test.err = double(17)
for(mtry in 1:17){
    fit = randomForest(popularity~., data = numeric_spotifydf, subset=train, mtry=mtry, ntree = 350)
      oob.err[mtry] = fit$mse[350]
      pred = predict(fit, numeric_spotifydf[-train,])
      test.err[mtry] = with(numeric_spotifydf[-train,], mean( (popularity-pred)^2 ))
}

```

Random Forest Regression Trees 17*350 trees with MSE for OOB and Test errors

```{r}
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error", lwd=6)
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))

```

The above plot suggests that Out-Of-Bag error estimates (red colored curve) is very far apart from test error estimates. There is no correlation between OOB and test errors. Errors never really tends to minimize as the number of features increase during training.

```{r}
oob.err 
test.err
```

Basic Random Forest on all features

```{r}
rf.spotify1 = randomForest(popularity~., data = numeric_spotifydf, subset = train, importance = TRUE)
rf.spotify1
summary(rf.spotify1)
```

```{r}
plot(rf.spotify1)
```


In case of Random Forest regression, number of trees are 500 with 4 variables at each split. as well the MSR is 268 and explained variance is still low 29.36 % of variance explained.

Use Random forest to find Variance based Feature Importances.

```{r}
rf.spotify1$importance
rf.spotify1$importanceSD
```

Plot the Feature importances from Random Forest Regression for each feature.

```{r}
create_rfplot <- function(rf, type){
  
  imp <- importance(rf, type = type, scale = F)
  
  featureImportance <- data.frame(Feature = row.names(imp), Importance = imp[,1])
  
  p <- ggplot(featureImportance, aes(x = reorder(Feature, Importance), y = Importance)) +
       geom_bar(stat = "identity", fill = featureImportance$Importance, width = 0.65) +
       coord_flip() + 
       theme_light(base_size = 25) +
       theme(axis.title.x = element_text(size = 20, color = "black"),
             axis.title.y = element_blank(),
             axis.text.x  = element_text(size = 20, color = "black"),
             axis.text.y  = element_text(size = 20, color = "black")) 
  return(p)
}
create_rfplot(rf.spotify1, type = 2)

```


The Random Forest Regression Feature importances suggest that duration is the most important feature of the response variable i.e Popularity score.

```{r}
data.frame(Feature = row.names(rf.spotify1$importance), Importance = rf.spotify1$importance[,1])
```

To do permutation Importance to compare Feature importances with that of Feature Importances from Random Forest regression's variance based importance, we need forest and inbag values to be available in RFR forest object so Permutation-based analysis over repeated samples of all-features-except-one is done based on available statistical information. This makes Permutation Importance more relevant. The Permutation Importance done here is also conditional since we observe multi-collinearity between predictors/independent features themselves. Conditional Permutation importance more relevant in this case since finding accuracy when correlation threshold is (suggested > 0.2) which is true in this case. We have multiple features having correlation > 0.2 suggesting Conditional Permutation Importance as more appropriate method to find importance of a feature's relation to response variable.

```{r}
rf.spotify2 = randomForest(popularity~., data = numeric_spotifydf, subset = train, replace = FALSE, nodesize = 7, keep.forest = TRUE, keep.inbag = TRUE)
rf.spotify2
permimp <- permimp(rf.spotify2, conditional = TRUE, progressBar = FALSE, do_check=FALSE)
```

## Permutation Importances

Permutation Importances are calculated by repeated bagging, bootstrapping and sampling from Random Forest Regression fit to derive importance of a feature based on change in error (positive or negative or none). If there was no change in error in absence of a feature, that feature is considered not important for observed values of reponse variable.


```{r}
permimp$values
```

```{r}
ggplot(as.data.frame(permimp$values), aes(x = reorder(names(permimp$values)
, as.numeric(permimp$values)), y = as.numeric(permimp$values))) +
       geom_bar(stat = "identity", fill = as.factor(seq(0,11)), width = 0.65) +
       coord_flip() + 
       theme_light(base_size = 25) +
       theme(axis.title.x = element_text(size = 20, color = "black"),
             axis.title.y = element_blank(),
             axis.text.x  = element_text(size = 20, color = "black"),
             axis.text.y  = element_text(size = 20, color = "black"))+xlab("Features")+ylab("Importance")
```


Lastly, we see that on the contrary, acousticness has no influence over popularity score. However, duration, instrumentalness, loudness, valence seem to have good influence over response variable.

Finally, we think given the number of tracks, we can use most statsitically important features to train and test regression fit using cross validation and epochs. Further we would like to conduct tests on a randomly generated date from the model fit. We further would like to explore, as a future work, to find regression or models that better  fit multi-collinear features while also finding some better techniques to normalize each of feature-wise distributions.


# Potential Bias and Conclusion


## Bias

Following are some of the bias affecting our data and causing an unreliable analysis.

1. Currently, the Spotify user base predominantly comprises European, North American and Latin America listeners[ref](“ https://www.statista.com/statistics/813902/spotify-share-monthly-active-users-by-region/”).  This heavily influences popularity charts and which songs get plays. This lack of diversity certainly affects our data set and makes it less nuanced.

2. There are over 70 million songs on Spotify, of which this data set includes only 230,000. Having feature values of these less popular songs would be immensely helpful to determine which songs have the potential to get famous.

3. On the analytics end, our modest learning models have a limited capability of predicting popularity with the provided variables. We might need to get more sophisticated with our modeling as we have only scratched the surface at this point of time and might need to apply fancier predictive.

4. A more robust dataset with added attributes, in addition to a more elegant model, would definitely prove more effective and help us achieve our end goals with more satisfactory results.

## Conclusion

Lastly, we see that on the contrary, acousticness has no influence over popularity score. However, duration, instrumentalness, loudness, valence seem to have good influence over response variable.
Finally, we think given the number of tracks, we can use most statistically important features to train and test regression fit using cross validation and epochs. Further we would like to conduct tests on a randomly generated date from the model fit. We further would like to explore, as a future work, to find regression or models that better fit multi-collinear features while also finding some better techniques to normalize each of feature-wise distributions. The additional information about musical and vocal acoustics and the pattern in which the acoustic signals themselves relate to the popularity score will be helpful

